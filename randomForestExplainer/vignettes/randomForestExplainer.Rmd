---
title: "Understanding random forests with randomForestExplainer"
author: "Ola PaluszyÅ„ska"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Understanding random forests with randomForestExplainer}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

# Introduction

This is a preliminary draft of the vignette for `randomForestExplainer` package. I will provide examples of usage of new functions as I create them to track package development.

I will use the following packages:

```{r}
library(data.table)
library(dplyr)
library(ggplot2)
library(randomForest)
library(randomForestExplainer)
```

# Data

I will use the data on Glioblastoma that contain only 125 observations but 16119 variables. The goal of my random forest will be to predict whether the patient died or not within a year from diagnosis (variable `death1y`).

```{r}
load("GlioblastomaWide.rda")
```

First I need to change our dependent variable into a factor and remove variables for which there are NAs (otherewise we would have to remove corresponding observations to build a random forest and our sample is very small anyway). Additionally, I remove the variable `sampleID` which is just the ID of the patient.

```{r}
GlioblastomaWide$death1y <- as.factor(GlioblastomaWide$death1y)
remove <- is.na(GlioblastomaWide) %>% colSums()
GlioblastomaWide <- GlioblastomaWide[, remove == 0]
rm(remove)
GlioblastomaWide <- GlioblastomaWide[, -1]
```

Now, I use the `randomForest` package to build a random forest for the data. I need the forest to have as many trees as possible, as with thousands of variables the probablility of each being considered for a split of a node is low and for analyzing the structure of the random forest I need a reasonable number of such instances for each variable, as I cannot say anything about the importance of a variable that was not even considered for a split. Moreover, we will analyze measures of importance of variables so we set `importance = TRUE`.

As building of the below forest takes a long time I load it from the memory.

```{r}
# forest <- randomForest(death1y ~ ., data = GlioblastomaWide, ntree = 10000, importance = TRUE)
# save(forest, file = "GlioblastomaWide_forest.rda")
load("GlioblastomaWide_forest.rda")
```

[here we will check if the random forest is a reasonable predictor]

What follows is a demonstration of how the new package works using this forest as an example. It is worth noting, that I distinguish functions that produce data from the ones that plot it (I always use `ggplot2` for that). Also, bear in mind that this dataset is of a particular type (many more variables than observations) and all functions adress issues connected to that, by restricting the results to only some variables, etc., which may be redundant in a low-dimensional setting.

# Distribution of minimal depth

First, I demonstrate new ways in which a random forest can be interpreted using the distribution of minimal depth across its trees. Below, I show how to use the two main functions provided in the file min_depth_distribution.R to explore my random forest.

To analyze the distribution of minimal depth we first need to calculate it. This is what function `min_depth_distribution` does -- it creates one data frame for the whole forest (tracks trees with column `tree`) and then for each tree calculates the minimal depth of all variables that were used for splitting within it. It takes the forest as its argument and returns a data frame with the distribution of minimal depth across trees.

```{r}
min_depth_frame <- min_depth_distribution(forest)
head(min_depth_frame, n = 10) # this is how the output looks like
```

It is worth noting, that using this function we only get data concerning the minimal depth for each variable that was used for splitting in a given tree. We do not fill the frame with NAs whenever a variable does not have a minimal depth in a tree (because it was not used for splitting in it) as when there are many variables (as is the case in our example) the resulting data frame would be enormous and one could easily run out of memory and get an error. In other cases it is simple to do with the following code:

```{r}
# library(tidyr)
# min_depth_frame_complete <- tidyr::complete(min_depth_frame, tree = 1:forest$ntree, variable = labels(forest$terms))
```

To visualize the distribution of minimal depth across trees, the function `plot_min_depth_distribution` plots the minimal depth distribution for a maximum of `k` variables with highest mean minimal depth (default is 10). For this, only variables that were used for splitting in at least `min_no_of_trees` number of trees are used -- this argument is of particular interest in cases with many variables like in our example, as some variables may have a low mean minimal depth only due to chance (e.g., they were present only in one out of 10000 trees and there happend to be used to splitting close to the root). The default value for this parameter is half of the maximal number of trees in which any variable was used for splitting (in our case this is half of 94).

```{r, fig.width = 7, fig.height = 5}
plot_min_depth_distribution(min_depth_frame)
```

The vertical bar indicates the mean minimal depth rescaled such that the maximal possible value is the maximal number of trees in which any variable was used for splitting (this is 94 in our case -- for variable SH3BP2). Note, that the number of NA's plotted is the number of NA's that a variable has on top of the minimal number of NA's for any variable in the dataset.

Below, we use the function again with different options:

```{r, fig.width = 7, fig.height = 7}
plot_min_depth_distribution(min_depth_frame, k = 15, min_no_of_trees = 50)
```

Of course, the result is very sensitive to changes in parameters `k` and `min_no_of_trees`. This is because we have so many variables and their occurance as split variables is generally rare (less than 0.1%). That is just another argument for looking at the whole distribution of minimal depth instead of mean minimal depth, especially for such datasets as this one.

# Multi-way importance plot

The next step in understanding a random forest is to compare the performance of variables across different measures of importance. This idea is implemented in file measure_importance.

First, the function `measure_importance` calculates all importance measures for our forest and returns them in a single data frame. This includes extracting measures implemented already by the `randomForest` package (mean decrease in accuracy and mean dearease in MSE for regression; mean decrease in accuracy for each class, for all classes and mean decrease in Gini index for classification) and calculating the following additional ones:

- *mean minimal depth*

- *number of nodes* in which a variale is used for splitting

- *number of trees* in which a variable is used for splitting

- *number of times the variable splits at node*

- *significance of the variable* [not added yet].

For our forest we get the following result:

```{r}
# importance_frame <- measure_importance(forest)
# save(importance_frame, file = "GlioblastomaWide_importance_frame.rda")
load("GlioblastomaWide_importance_frame.rda")
head(importance_frame, n = 10)
```

The next step is to plot two or three of those measures against each other. This is done using the `multi_way_importance_plot` function, which takes the following arguments:

- `importance_frame` -- the frame with importance measures,

- `x_measure, y_measure, size_measure` -- importance measures to be presented as dimensions x, y and size of points of the scatter plot; in case the number of nodes or trees are chosen as either x or y measure, the square root scale is used,

- `min_no_of_trees` -- the minimal number of trees in which a variable has to be used for splitting to be included in the plot; the default is 10% of the maximal number of trees in which a variable appears,

- `x_label_prob, y_label_prob, size_label_prob` -- the fraction of top variables according to each dimension in which a variable has to be to be labeled on the plot (a variable needs to be within the top fraction for every measure to be labeled); the default is 0.01 for each,

- `main` -- title of the plot; the default is "Multi-way importance plot".

Below, we use the Multi-way importance plot to visualize our forest for four different sets of parameters:

```{r, fig.width = 7, fig.height = 7}
# Possible measures: "mean_minimal_depth", "no_of_nodes", "accuracy_decrease", "gini_decrease", "no_of_trees", "times_a_root"
multi_way_importance_plot(importance_frame, x_measure = "mean_minimal_depth", y_measure = "no_of_trees", 
                          size_measure = "gini_decrease", min_no_of_trees = 0.2*max(importance_frame$no_of_trees),
                          x_label_prob = 0.01, y_label_prob = 0.01, size_label_prob = 0.01, main = "Multi-way importance plot")
multi_way_importance_plot(importance_frame, x_measure = "mean_minimal_depth", y_measure = "no_of_nodes", 
                          size_measure = "accuracy_decrease", min_no_of_trees = 0.2*max(importance_frame$no_of_trees),
                          x_label_prob = 0.01, y_label_prob = 0.01, size_label_prob = 0.01, main = "Multi-way importance plot")
```

Note, that the above plots are almost identical -- that is because the number of nodes is in `r sum(importance_frame$no_of_trees == importance_frame$no_of_nodes)` out of `r nrow(importance_frame)` cases identical to the number of trees, as the trees in our forest are pretty short. This might not be the case for other datasets and either of the two measures might be preffered (they can also be compared using this plot).

One could also consider the number of times the variable was used for splitting at the root or rearrange the dimensions and reduce the number of points plotted:

```{r, fig.width = 7, fig.height = 7}
multi_way_importance_plot(importance_frame, x_measure = "mean_minimal_depth", y_measure = "times_a_root", 
                          size_measure = "gini_decrease", min_no_of_trees = 0.2*max(importance_frame$no_of_trees),
                          x_label_prob = 0.01, y_label_prob = 0.01, size_label_prob = 0.01, main = "Multi-way importance plot")
multi_way_importance_plot(importance_frame, x_measure = "mean_minimal_depth", y_measure = "gini_decrease", 
                          size_measure = "times_a_root", min_no_of_trees = 0.4*max(importance_frame$no_of_trees),
                          x_label_prob = 0.01, y_label_prob = 0.01, size_label_prob = 0.01, main = "Multi-way importance plot")
```

# Other examples

## Multi-class classification: breast cancer data

Download and process data:

```{r}
# source("https://bioconductor.org/biocLite.R")
# biocLite("DESeq")
# biocLite("limma")
# biocLite("TxDb.Hsapiens.UCSC.hg18.knownGene")
# biocLite("org.Hs.eg.db")
# biocLite("DESeq2")
# biocLite("edgeR")
# devtools::install_github("geneticsMiNIng/MLGenSig", subdir = "MetExpR")

# brca <- MetExpR::BRCA_mRNAseq_chr17
# colnames(brca) <- make.names(colnames(brca))
# brca$SUBTYPE <- factor(brca$SUBTYPE)

# save(brca, file = "BreastCancer.rda")
load("BreastCancer.rda")
```

Build a forest:

```{r}
# set.seed(2017)
# forest_brca <- randomForest(SUBTYPE ~ ., data = brca, ntree = 10000, importance = TRUE, localImp = TRUE)
# save(forest_brca, file = "BreastCancer_forest.rda")
load("BreastCancer_forest.rda")
```

## Regression: PISA data

Download and process data:

```{r}
# devtools::install_github("pbiecek/PISA2012lite")
# library("PISA2012lite")

# pisa <- na.omit(student2012[,c(1, 4, 12, 13, 18:20, 39, 61:62, 114, 488, 457, 501)])
# pisa <- pisa[pisa$CNT %in% c("Austria", "Belgium", "Czech Republic", "Germany", "Denmark", "Spain", "Estonia", "Finland", "France", "United Kingdom", "Greece", "Hungary", "Ireland", "Italy", "Lithuania", "Luxembourg", "Latvia", "Netherlands", "Poland", "Portugal", "Slovak Republic", "Slovenia", "Sweden", "Romania", "Croatia", "Bulgaria"),] # consider only EU countries to reduce the size
# pisa <- pisa[pisa$CNT %in% c(), ] # only the Visegrad group to begin with
# pisa$CNT <- factor(pisa$CNT)

# save(pisa, file = "PISA.rda")
load("PISA.rda")
```

Build a forest:

```{r}
# set.seed(2017)
forest_pisa <- randomForest(PV1MATH ~ ., data = pisa, importance = TRUE, localImp = TRUE)
save(forest_pisa, file = "PISA_forest.rda")
load("PISA_forest.rda")
```





