---
title: "Understanding random forests with randomForestExplainer"
author: "Ola PaluszyÅ„ska"
output: 
  rmarkdown::html_document:
    toc: true
    toc_float: true
vignette: >
  %\VignetteIndexEntry{Understanding random forests with randomForestExplainer}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

# Introduction

This is a preliminary draft of the vignette for `randomForestExplainer` package. I will provide examples of usage of new functions as I create them to track package development.

I will use the following packages:

```{r}
library(randomForest)
library(randomForestExplainer)
```

# Data

I will use the data on Glioblastoma that contain only 125 observations but 16119 variables. The goal of my random forest will be to predict whether the patient died or not within a year from diagnosis (variable `death1y`).

```{r}
load("GlioblastomaWide.rda")
```

First I need to change our dependent variable into a factor and remove variables for which there are NAs (otherewise we would have to remove corresponding observations to build a random forest and our sample is very small anyway). Additionally, I remove the variable `sampleID` which is just the ID of the patient.

```{r}
GlioblastomaWide <- GlioblastomaWide[, -1]
GlioblastomaWide$death1y <- as.factor(GlioblastomaWide$death1y)
remove <- is.na(GlioblastomaWide) %>% colSums()
GlioblastomaWide <- GlioblastomaWide[, remove == 0]
rm(remove)
```

Now, I use the `randomForest` package to build a random forest for the data. I need the forest to have as many trees as possible, as with thousands of variables the probablility of each being considered for a split of a node is low and for analyzing the structure of the random forest I need a reasonable number of such instances for each variable, as I cannot say anything about the importance of a variable that was not even considered for a split. Moreover, we will analyze measures of importance of variables so we set `importance = TRUE`.

As building of the below forest takes a long time I load it from the memory.

```{r}
# set.seed(2017)
# forest <- randomForest(death1y ~ ., data = GlioblastomaWide, ntree = 10000, localImp = TRUE)
# save(forest, file = "GlioblastomaWide_forest.rda")
load("GlioblastomaWide_forest.rda")
```

[here we will check if the random forest is a reasonable predictor]

What follows is a demonstration of how the new package works using this forest as an example. It is worth noting, that I distinguish functions that produce data from the ones that plot it (I always use `ggplot2` for that). Also, bear in mind that this dataset is of a particular type (many more variables than observations) and all functions adress issues connected to that, by restricting the results to only some variables, etc., which may be redundant in a low-dimensional setting.

# Distribution of minimal depth

First, I demonstrate new ways in which a random forest can be interpreted using the distribution of minimal depth across its trees. Below, I show how to use the two main functions provided in the file min_depth_distribution.R to explore my random forest.

To analyze the distribution of minimal depth we first need to calculate it. This is what function `min_depth_distribution` does -- it creates one data frame for the whole forest (tracks trees with column `tree`) and then for each tree calculates the minimal depth of all variables that were used for splitting within it. It takes the forest as its argument and returns a data frame with the distribution of minimal depth across trees.

```{r}
min_depth_frame <- min_depth_distribution(forest)
head(min_depth_frame, n = 10) # this is how the output looks like
```

It is worth noting, that using this function we only get data concerning the minimal depth for each variable that was used for splitting in a given tree. We do not fill the frame with NAs whenever a variable does not have a minimal depth in a tree (because it was not used for splitting in it) as when there are many variables (as is the case in our example) the resulting data frame would be enormous and one could easily run out of memory and get an error. In other cases it is simple to do with the following code:

```{r}
# library(tidyr)
# min_depth_frame_complete <- tidyr::complete(min_depth_frame, tree = 1:forest$ntree, variable = labels(forest$terms))
```

To visualize the distribution of minimal depth across trees, the function `plot_min_depth_distribution` plots the minimal depth distribution for a maximum of `k` variables with highest mean minimal depth (default is 10). For this, only variables that were used for splitting in at least `min_no_of_trees` number of trees are used -- this argument is of particular interest in cases with many variables like in our example, as some variables may have a low mean minimal depth only due to chance (e.g., they were present only in one out of 10000 trees and there happend to be used to splitting close to the root). The default value for this parameter is half of the maximal number of trees in which any variable was used for splitting (in our case this is half of 94).

```{r, fig.width = 7, fig.height = 5}
plot_min_depth_distribution(min_depth_frame)
```

The vertical bar indicates the mean minimal depth rescaled such that the maximal possible value is the maximal number of trees in which any variable was used for splitting (this is 94 in our case -- for variable SH3BP2). Note, that the number of NA's plotted is the number of NA's that a variable has on top of the minimal number of NA's for any variable in the dataset.

Below, we use the function again with different options:

```{r, fig.width = 7, fig.height = 7}
plot_min_depth_distribution(min_depth_frame, min_no_of_trees = 60, mean_sample = "relevant_trees")
```

Of course, the result is very sensitive to changes in parameters `k` and `min_no_of_trees`. This is because we have so many variables and their occurance as split variables is generally rare (less than 0.1%). That is just another argument for looking at the whole distribution of minimal depth instead of mean minimal depth, especially for such datasets as this one.

# Measures of importance of variables

The next step in understanding a random forest is to compare the performance of variables across different measures of importance. This idea is implemented in file measure_importance.

## Measure importance

First, the function `measure_importance` calculates all importance measures for our forest and returns them in a single data frame. This includes extracting measures implemented already by the `randomForest` package (mean decrease in accuracy and mean dearease in MSE for regression; mean decrease in accuracy for  all classes and mean decrease in Gini index for classification) and calculating the following additional ones:

- *mean minimal depth*

- *number of nodes* in which a variale is used for splitting

- *number of trees* in which a variable is used for splitting

- *number of times the variable splits at node*

- *significance of the variable* p-value for the one-sided binomial test in which success means that a variable was used for splitting at a node, the number of trials is the total number of nodes and the probability of the split is equal to $$\mathbb{P}(\text{node splits on x}) = \mathbb{P}(\{x is among the variables tried})\mathbb{P}(\text{x is selected}) = \frac{\text{mtry}}{\text{no of variables}} \frac{1}{\text{mtry}} = \frac{1}{\text{no of variables}}.$$

For our forest we get the following result:

```{r}
# importance_frame <- measure_importance(forest)
# save(importance_frame, file = "GlioblastomaWide_importance_frame.rda")
load("GlioblastomaWide_importance_frame.rda")
head(importance_frame, n = 10)
```

## Multi-way importance plot

The next step is to plot two or three of those measures against each other. This is done using the `plot_multi_way_importance` function, which takes the following arguments:

- `importance_frame` -- the frame with importance measures,

- `x_measure, y_measure, size_measure` -- importance measures to be presented as dimensions x, y and size of points of the scatter plot; in case the number of nodes or trees are chosen as either x or y measure, the square root scale is used,

- `min_no_of_trees` -- the minimal number of trees in which a variable has to be used for splitting to be included in the plot; the default is 10% of the maximal number of trees in which a variable appears,

- `no_of_labels` -- the number of top variables (according to the measures plotted) to label and highliht on the plot; the default is 10,

- `main` -- title of the plot; the default is "Multi-way importance plot".

Below, we use the Multi-way importance plot to visualize our forest for four different sets of parameters:

```{r, fig.width = 10, fig.height = 5}
# Possible measures for classification: "mean_min_depth", "no_of_nodes", "accuracy_decrease", "gini_decrease", "no_of_trees", "times_a_root", "p_value"
plot_multi_way_importance(importance_frame, size_measure = "no_of_nodes", min_no_of_trees = 30)
plot_multi_way_importance(importance_frame, x_measure = "accuracy_decrease", y_measure = "gini_decrease", size_measure = "p_value")
```

Note, that the above plots are almost identical -- that is because the number of nodes is in `r sum(importance_frame$no_of_trees == importance_frame$no_of_nodes)` out of `r nrow(importance_frame)` cases identical to the number of trees, as the trees in our forest are pretty short. This might not be the case for other datasets and either of the two measures might be preffered (they can also be compared using this plot).

One could also consider the number of times the variable was used for splitting at the root or rearrange the dimensions and reduce the number of points plotted:

```{r, fig.width = 7, fig.height = 7}
plot_multi_way_importance(importance_frame, x_measure = "mean_min_depth", 
                          y_measure = "times_a_root", size_measure = "p_value", 
                          min_no_of_trees = 0.2*max(importance_frame$no_of_trees),
                          no_of_labels = 10, 
                          main = "Multi-way importance plot")
plot_multi_way_importance(importance_frame, x_measure = "mean_min_depth", 
                          y_measure = "gini_decrease", size_measure = "times_a_root", 
                          min_no_of_trees = 0.4*max(importance_frame$no_of_trees),
                          no_of_labels = 15, 
                          main = "Multi-way importance plot")
```

## Compare measures using ggpairs

One drawback of the muli-way importance plot is that it only shows three importance measures (and only the values of the ones shown on the axes can be precisely read from the plot), whereas we have a few more of them and it is never clear which ones are the most meaningful (this will also vary depending on the application).

To show more measures and relations between them at a time the function `plot_importance_ggpairs` uses the `GGally::ggpairs` function to plot two-way scatterplots for selected measures (supplied in a character vector), the default uses the following four:

```{r, fig.width = 7, fig.height = 6}
plot_importance_ggpairs(importance_frame)
```

## Compare different rankings

Another approach would be to rank the variables according to each of those measures and use the rankings for plotting. This helps identify the measures for which the rankings align. For this we use the `plot_importance_rankings` function:

```{r, fig.width = 7, fig.height = 6}
plot_importance_rankings(importance_frame)
```

As we can see, in applications with so many variables as here these kinds of plots tend to be pretty messy, but we can still identify the measures that produce similar rankings.

# Variable interactions

Up untill now we only analyzed single variables trying to get a sense of how important they are in a forest. A natural extension of that approach would be to consider interactions of variables. To do that, we need to pick a subset of variables with which we will interact the other ones (the trees in a forest have a hierarchical structure, so when we consider an interaction of two variables, one is used for splitting higher in the tree than the other, if at all). These should be variables somewhat important in the forest on their own so we use importance measures to select them using the function `important_variables()` which takes as arguments your importance frame, the number of variables that you want to get and the measures on the basis of which the variables should be selected (the function ranks variables according to those measures and than uses the sum of those rankings):

```{r}
vars <- important_variables(importance_frame, k = 20, measures = c("mean_min_depth", "no_of_trees"))
```

## Conditional minimal depth

### Example: our forest

The easiest extension of what we already used to the case of interactions is measuring minimal depth as one can easily define conditional minimal depth of one variable with respect to the other. What we do is in each tree we find the maximal subtree with a root splitting on one of the selected variables and then calculate depths of other variables within that subtree to finally get mean minimal depth for each such interaction. This is calculated by the function `min_depth_interactions`, which usually takes a while:

```{r}
# interactions_frame <- min_depth_interactions(forest, vars)
# save(interactions_frame, file = "GlioblastomaWide_interactions_frame.rda")
load("GlioblastomaWide_interactions_frame.rda")
```

In our case the result looks like this:

```{r}
head(interactions_frame)
```

As we can suspect many interactions do not occur at all (we have a lot of variables and the trees are very short due to a small number of observations), so we should look at those that do:

```{r}
head(interactions_frame[order(interactions_frame$occurrences, decreasing = TRUE), ])
```

The most frequent interaction in this forest occured only three times, which makes the analysis of interactions meaningless. Therefore, we build another random forest for our data with increased `mtry` parameter, which increases the chances of picking each variable for splitting at every node -- hopefully this will make meaningful interactions appear more frequently. We also select the conditioning variables based on the new importance measures table.

### Example: a forest with higher mtry

```{r}
# set.seed(2017)
# forest_v2 <- randomForest(death1y ~ ., data = GlioblastomaWide, ntree = 10000, mtry = floor(ncol(GlioblastomaWide)/3), importance = TRUE, localImp = TRUE)
# save(forest_v2, file = "GlioblastomaWide_forest_v2.rda")
load("GlioblastomaWide_forest_v2.rda"); rm(forest)
importance_frame <- measure_importance(forest_v2)
vars <- important_variables(importance_frame, k = 20, measures = c("mean_min_depth", "no_of_trees"))
# interactions_frame <- min_depth_interactions(forest_v2, vars)
# save(interactions_frame, file = "GlioblastomaWide_interactions_frame_v2.rda")
load("GlioblastomaWide_interactions_frame_v2.rda")
head(interactions_frame[order(interactions_frame$occurrences, decreasing = TRUE), ])
```

As we can see in the new forest the most frequent interaction occurs 29 times, so analyzing interactions starts to make sens and we can show the ones with lowest mean minimal depth on a plot using the  `interactions_frame` function, which compares the mean minimal depth of most frequent interaction to its minimal value among interactions:

```{r, fig.width = 7, fig.height = 5}
plot_min_depth_interactions(interactions_frame)
```

It is worth noting, that interaction analysis is not such a good idea for this particular dataset as we get short trees in the forest. However, it can be very important in applications with higher trees and that is why we include such examples in the end.

# Summarize forest

The `summarize_forest()` function is the flagship function of the `randomForestExplainer` package, as it takes your random forest and produces a html report that summarizes all basic results obtained for the forest with the new package. Below, we show the result of running this function with default settings (we only supply the forest and sett interactions = TRUE contrary to the default to show full functionality).

## Glioblastoma forest: our main example

To explore the importance of variables using all functionality of `randomForestExplainer` we just need to run the following:

```{r, eval = FALSE}
summarize_forest(forest, interactions = TRUE)
```

Now, in the working directory a file "Summary_of_your_forest.html" appears and it looks like this:
[Glioblastoma forest summary](https://rawgit.com/geneticsMiNIng/BlackBoxOpener/master/examples/Glioblastoma_summary.html)

## Glioblastoma forest with increased mtry

We repeat this for the second version of our forest:

```{r, eval = FALSE}
summarize_forest(forest_v2, interactions = TRUE)
```

[Glioblastoma forest version 2 summary](https://rawgit.com/geneticsMiNIng/BlackBoxOpener/master/examples/Glioblastoma_summary_v2.html)

## Multi-class classification: breast cancer data

We use the breast cancer data as an example of application of random forests to multi-class classification problems. To download and process the data we execute we executed the following code:

```{r, eval = FALSE}
# source("https://bioconductor.org/biocLite.R")
# biocLite("DESeq")
# biocLite("limma")
# biocLite("TxDb.Hsapiens.UCSC.hg18.knownGene")
# biocLite("org.Hs.eg.db")
# biocLite("DESeq2")
# biocLite("edgeR")
# devtools::install_github("geneticsMiNIng/MLGenSig", subdir = "MetExpR")

# brca <- MetExpR::BRCA_mRNAseq_chr17
# colnames(brca) <- make.names(colnames(brca))
# brca$SUBTYPE <- factor(brca$SUBTYPE)

# save(brca, file = "BreastCancer.rda")
load("BreastCancer.rda")
```

Now, we build the random forest with an increased number of trees due to a big number of variables. As this takes a while we retrieve the result from memory and summarize it:

```{r, eval = FALSE}
# set.seed(2017)
# forest_brca <- randomForest(SUBTYPE ~ ., data = brca, ntree = 10000, localImp = TRUE)
# save(forest_brca, file = "BreastCancer_forest.rda")
load("BreastCancer_forest.rda")
summarize_forest(forest_brca, interactions = TRUE)
```

[Breast cancer summary](https://rawgit.com/geneticsMiNIng/BlackBoxOpener/master/examples/BreastCancer_summary.html)

## Regression: PISA data

The same works for the PISA 2015 dataset, for which we consider regression. To download and process the data we executed the following code:

```{r, eval = FALSE}
# devtools::install_github("pbiecek/PISA2012lite")
# library("PISA2012lite")

# pisa <- na.omit(student2012[,c(1, 4, 12, 13, 18:20, 39, 61:62, 114, 488, 457, 501)])
# pisa <- pisa[pisa$CNT %in% c("Austria", "Belgium", "Czech Republic", "Germany", "Denmark", "Spain", "Estonia", "Finland", "France", "United Kingdom", "Greece", "Hungary", "Ireland", "Italy", "Lithuania", "Luxembourg", "Latvia", "Netherlands", "Poland", "Portugal", "Slovak Republic", "Slovenia", "Sweden", "Romania", "Croatia", "Bulgaria"),] # consider only EU countries to reduce the size
# save(pisa, file = "PISA.rda")
load("PISA.rda")

# Further reduce the size
pisa <- pisa[pisa$CNT %in% c("Czech Republic", "Hungary", "Poland", "Slovak Republic"), ] # only the Visegrad group
pisa$CNT <- factor(pisa$CNT)
```

We build a forest and summarize it:

```{r, eval = FALSE}
# set.seed(2017)
# forest_pisa <- randomForest(PV1MATH ~ ., data = pisa, localImp = TRUE)
# save(forest_pisa, file = "PISA_forest.rda")
load("PISA_forest.rda")
summarize_forest(forest_pisa, interactions = TRUE)
```

[PISA forest summary](https://rawgit.com/geneticsMiNIng/BlackBoxOpener/master/examples/PISA_summary.html)

## Regression: Boston data

To easily compare the results of using `randomForestExplainer` to the ones obtained by `ggRandomForests`, another package dealing with random forest visualisation, we also consider the regression example for the `Boston` dataset from package `MASS`, which was used in the package vignette for `ggRandomForests`. We do that with the following code:

```{r, eval = FALSE}
# data(Boston, package = "MASS")
# Boston$chas <- as.logical(Boston$chas)
# set.seed(2017)
# forest_Boston <- randomForest(medv ~ ., data = Boston, ntree = 1000, localImp = TRUE)
# save(forest_Boston, file = "Boston_forest.rda")
load("Boston_forest.rda")
summarize_forest(forest_Boston, interactions = TRUE)
```

[Boston forest summary](https://rawgit.com/geneticsMiNIng/BlackBoxOpener/master/examples/Boston_summary.html)


